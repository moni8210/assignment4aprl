{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc666d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the required details\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "# Extract the required details: Rank, Name, Artist, Upload date, Views\n",
    "video_details = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    rank = cells[0].text.strip()\n",
    "    name = cells[1].text.strip()\n",
    "    artist = cells[2].text.strip()\n",
    "    upload_date = cells[4].text.strip()\n",
    "    views = cells[3].text.strip()\n",
    "    video_details.append({\n",
    "        'Rank': rank,\n",
    "        'Name': name,\n",
    "        'Artist': artist,\n",
    "        'Upload date': upload_date,\n",
    "        'Views': views\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for video in video_details:\n",
    "    print(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade51c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launch web browser and navigate to the page\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver = webdriver.Chrome()  # You may need to download the appropriate webdriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the international fixtures option\n",
    "driver.find_element_by_css_selector(\"li.nav-item[tabindex='0']:nth-of-type(2)\").click()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the required details: Series, Place, Date, Time\n",
    "fixture_details = []\n",
    "fixtures = soup.find_all('div', class_='js-list')\n",
    "for fixture in fixtures:\n",
    "    series = fixture.find('span', class_='u-unskewed-text').text.strip()\n",
    "    place = fixture.find('p', class_='fixture__additional-info').text.strip()\n",
    "    date = fixture.find('span', class_='fixture__date').text.strip()\n",
    "    time = fixture.find('span', class_='fixture__time').text.strip()\n",
    "    fixture_details.append({\n",
    "        'Series': series,\n",
    "        'Place': place,\n",
    "        'Date': date,\n",
    "        'Time': time\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for fixture in fixture_details:\n",
    "    print(fixture)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a71313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the statisticstimes home page to reach the economy page\n",
    "url = \"http://statisticstimes.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find and click on the economy page link\n",
    "economy_link = soup.find('a', text='Economy')\n",
    "economy_url = economy_link['href']\n",
    "economy_response = requests.get(economy_url)\n",
    "economy_soup = BeautifulSoup(economy_response.text, 'html.parser')\n",
    "\n",
    "# Find and click on the GDP of Indian states link\n",
    "gdp_link = economy_soup.find('a', text='GDP of Indian states')\n",
    "gdp_url = gdp_link['href']\n",
    "gdp_response = requests.get(gdp_url)\n",
    "gdp_soup = BeautifulSoup(gdp_response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the required details\n",
    "table = gdp_soup.find('table', class_='display')\n",
    "\n",
    "# Extract the required details: Rank, State, GSDP(18-19), GSDP(19-20), Share(18-19), GDP($ billion)\n",
    "gdp_details = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    rank = cells[0].text.strip()\n",
    "    state = cells[1].text.strip()\n",
    "    gsdp_18_19 = cells[2].text.strip()\n",
    "    gsdp_19_20 = cells[3].text.strip()\n",
    "    share_18_19 = cells[4].text.strip()\n",
    "    gdp_billion = cells[5].text.strip()\n",
    "    gdp_details.append({\n",
    "        'Rank': rank,\n",
    "        'State': state,\n",
    "        'GSDP(18-19)': gsdp_18_19,\n",
    "        'GSDP(19-20)': gsdp_19_20,\n",
    "        'Share(18-19)': share_18_19,\n",
    "        'GDP($ billion)': gdp_billion\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for gdp in gdp_details:\n",
    "    print(gdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e95ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Launch web browser and navigate to the page\n",
    "url = \"https://github.com/\"\n",
    "driver = webdriver.Chrome()  # You may need to download the appropriate webdriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the trending option from Explore menu\n",
    "explore_menu = driver.find_element_by_xpath(\"//summary[contains(text(), 'Explore')]\")\n",
    "explore_menu.click()\n",
    "time.sleep(1)  # Wait for the menu to expand\n",
    "trending_option = driver.find_element_by_xpath(\"//a[contains(text(), 'Trending')]\")\n",
    "trending_option.click()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the trending repositories\n",
    "repositories = soup.find_all('article', class_='Box-row')\n",
    "repo_details = []\n",
    "for repo in repositories:\n",
    "    title = repo.find('h1', class_='h3 lh-condensed').text.strip()\n",
    "    description = repo.find('p', class_='col-9 color-text-secondary my-1 pr-4').text.strip()\n",
    "    contributors = repo.find('a', class_='Link--muted d-inline-block mr-3').text.strip()\n",
    "    language = repo.find('span', itemprop='programmingLanguage').text.strip() if repo.find('span', itemprop='programmingLanguage') else None\n",
    "    repo_details.append({\n",
    "        'Repository title': title,\n",
    "        'Repository description': description,\n",
    "        'Contributors count': contributors,\n",
    "        'Language used': language\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for repo in repo_details:\n",
    "    print(repo)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48af7c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launch web browser and navigate to the page\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver = webdriver.Chrome()  # You may need to download the appropriate webdriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the charts option then hot 100-page link\n",
    "driver.find_element_by_xpath(\"//nav[@class='header__subnav bg--light flex--grow']/ul/li[3]\").click()\n",
    "driver.find_element_by_xpath(\"//a[contains(text(), 'Hot 100')]\").click()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the table containing the required details\n",
    "table = soup.find('table', class_='chart-detail')\n",
    "\n",
    "# Extract the required details: Song name, Artist name, Last week rank, Peak rank, Weeks on board\n",
    "song_details = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    song_name = cells[1].find('span', class_='chart-element_information_song text--truncate color--primary').text.strip()\n",
    "    artist_name = cells[1].find('span', class_='chart-element_information_artist text--truncate color--secondary').text.strip()\n",
    "    last_week_rank = cells[2].find('span', class_='text--last').text.strip()\n",
    "    peak_rank = cells[2].find('span', class_='text--peak').text.strip()\n",
    "    weeks_on_board = cells[2].find('span', class_='text--week').text.strip()\n",
    "    song_details.append({\n",
    "        'Song name': song_name,\n",
    "        'Artist name': artist_name,\n",
    "        'Last week rank': last_week_rank,\n",
    "        'Peak rank': peak_rank,\n",
    "        'Weeks on board': weeks_on_board\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for song in song_details:\n",
    "    print(song)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a36398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 6\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Fetch the webpage\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the required details\n",
    "table = soup.find('table', class_='in-article sortable')\n",
    "\n",
    "# Extract the required details: Book name, Author name, Volumes sold, Publisher, Genre\n",
    "novel_details = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    book_name = cells[0].text.strip()\n",
    "    author_name = cells[1].text.strip()\n",
    "    volumes_sold = cells[2].text.strip()\n",
    "    publisher = cells[3].text.strip()\n",
    "    genre = cells[4].text.strip()\n",
    "    novel_details.append({\n",
    "        'Book name': book_name,\n",
    "        'Author name': author_name,\n",
    "        'Volumes sold': volumes_sold,\n",
    "        'Publisher': publisher,\n",
    "        'Genre': genre\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for novel in novel_details:\n",
    "    print(novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 8\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Launch web browser and navigate to the page\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver = webdriver.Chrome()  # You may need to download the appropriate webdriver for your browser\n",
    "driver.get(url)\n",
    "\n",
    "# Click on the Show All Dataset page\n",
    "driver.find_element_by_xpath(\"//b[contains(text(), 'Show All Dataset')]\").click()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# Find the table containing the required details\n",
    "table = soup.find('table', cellpadding=\"3\")\n",
    "\n",
    "# Extract the required details: Dataset name, Data type, Task, Attribute type, No of instances, No of attribute, Year\n",
    "dataset_details = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    cells = row.find_all('td')\n",
    "    dataset_name = cells[0].text.strip()\n",
    "    data_type = cells[1].text.strip()\n",
    "    task = cells[2].text.strip()\n",
    "    attribute_type = cells[3].text.strip()\n",
    "    no_of_instances = cells[4].text.strip()\n",
    "    no_of_attributes = cells[5].text.strip()\n",
    "    year = cells[6].text.strip()\n",
    "    dataset_details.append({\n",
    "        'Dataset name': dataset_name,\n",
    "        'Data type': data_type,\n",
    "        'Task': task,\n",
    "        'Attribute type': attribute_type,\n",
    "        'No of instances': no_of_instances,\n",
    "        'No of attributes': no_of_attributes,\n",
    "        'Year': year\n",
    "    })\n",
    "\n",
    "# Print the scraped details\n",
    "for dataset in dataset_details:\n",
    "    print(dataset)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2725706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
